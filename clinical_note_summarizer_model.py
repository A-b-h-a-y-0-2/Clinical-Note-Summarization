# -*- coding: utf-8 -*-
"""Clinical Note Summarizer model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C7dmLHFQE4K1rFZbncxONs8Ynqjv4X_Z
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/cleaned_output_dropped.csv /content/drive/MyDrive/data-for-clinical-note-summarizer/data-for-clinical-note-summarizer/

!pip install datasets transformers pandas

pip install --upgrade datasets

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import TrainingArguments, Trainer

# data = pd.read_csv('/content/drive/MyDrive/data-for-clinical-note-summarizer/data-for-clinical-note-summarizer/cleaned_output_dropped.csv')

# df = data.drop(columns=['abstract', 'paragraphs'])

# Verify the DataFrame
# print(df.head())
# print(df.columns)

# Save the modified DataFrame to a new CSV file (optional)
output_file_path = "/content/cleaned_output_dropped.csv"
df.to_csv(output_file_path, index=False)

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/data-for-clinical-note-summarizer/data-for-clinical-note-summarizer/cleaned_output_dropped.csv')
train_df, val_df, test_df = (
    data.sample(frac=0.8, random_state=42),
    data.sample(frac=0.1, random_state=42),
    data.sample(frac=0.1, random_state=42)
)

data.columns

len(df['abstract_cleaned'])

from datasets import Dataset, DatasetDict
# Convert to Hugging Face Dataset
def load_dataset(df):
    return Dataset.from_pandas(df[['abstract_cleaned', 'paragraphs_cleaned']])

datasets = DatasetDict({
    "train": load_dataset(train_df),
    "validation": load_dataset(val_df),
    "test": load_dataset(test_df)
})

model_name = "GanjinZero/biobart-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization
def preprocess_data(examples):
    inputs = tokenizer(
        examples['paragraphs_cleaned'], max_length=512, truncation=True, padding='max_length'
    )
    targets = tokenizer(
        examples['abstract_cleaned'], max_length=150, truncation=True, padding='max_length'
    )
    inputs['labels'] = targets['input_ids']
    return inputs

# Apply tokenization
tokenized_datasets = datasets.map(preprocess_data, batched=True, remove_columns=['paragraphs_cleaned', 'abstract_cleaned'])

# Data Collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)

import torch
from transformers import TrainingArguments, Trainer

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ensure that the model is on the correct device
model = model.to(device)

from transformers import TrainingArguments, Trainer

# Training Arguments
training_args = TrainingArguments(
    output_dir="./bio_bart_summarizer",  # Directory to save checkpoints
    evaluation_strategy="epoch",        # Evaluate after every epoch
    save_strategy="epoch",              # Save model checkpoints after every epoch
    logging_dir="./logs",               # Directory for logging
    logging_steps=50,                   # Log every 50 steps
    per_device_train_batch_size=4,      # Batch size for training
    per_device_eval_batch_size=4,       # Batch size for evaluation
    gradient_accumulation_steps=2,      # Accumulate gradients to simulate a larger batch size
    learning_rate=5e-5,                 # Learning rate
    num_train_epochs=3,                 # Number of epochs
    weight_decay=0.01,                  # Weight decay for regularization
    save_total_limit=2,                 # Save only the last 2 checkpoints
    load_best_model_at_end=True,
        report_to=["none"],                  # Disable WandB logging if not needed
)

# Trainer Initialization
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer
)

# Train the Model
trainer.train()

# Save the Fine-Tuned Model and Tokenizer
model.save_pretrained("/content/drive/MyDrive/data-for-clinical-note-summarizer/fine_tuned_bio_bart")
tokenizer.save_pretrained("/content/drive/MyDrive/data-for-clinical-note-summarizer/fine_tuned_bio_bart")

print("Fine-tuned model saved successfully.")

!cp -r /content/bio_bart_summarizer /content/drive/MyDrive/data-for-clinical-note-summarizer/